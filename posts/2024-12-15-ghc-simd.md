---
title: Some thoughts on GHC's SIMD primitives
---

The use of parallelization is essential to exploit the performance of modern computers.
In particular, we want to take advantage of SIMD, multi-core, and GPUs.
This article will focus on SIMD.

GHC has provided primitives for SIMD for years, but they have not been effectively utilized.

In this article, I'd like to discuss what can be done to promote the utilization of GHC's SIMD primitives.

Disclaimer: I'm not an expert on high-performance computing.
I like exploiting hardware features for fun.

## Current status

### Paper

I must mention the following paper, which was written by the people who implemented SIMD primitives into GHC.

Geoffrey Mainland, Roman Leshchinskiy and Simon Peyton Jones, Exploiting Vector Instructions with Generalized Stream Fusion, ICFP 2013 

In addition to GHC's SIMD primitives, they seem to have considered working with the `vector` package and Data Parallel Haskell (DPH), but the integration with `vector` package doesn't seem to have happended, and DPH did not survive.

### Design

When exposing SIMD APIs to programming languages, there is the question of how to abstract (or not to abstract) the architecture-dependent vector register width.

GHC provides 128-bit, 256-bit, and 512-bit wide types, respectively.
They correspond directly to the corresponding CPU registers.
In other words, on x86 systems, 512-bit wide vector types correspond to ZMM (AVX-512) registers and do not use two YMM registers.

On the other hand, LLVM's abstraction allows vectors of arbitrary length to be written, and those longer than the SIMD width of the machine are compiled into multiple instructions/registers.

In the current design of GHC, the library will be responsible for abstracting the architecture-dependent SIMD width.

### Library

Since GHC's SIMD primitive types are unboxed, you would want boxed wrappers for daily use.
With a Hackage search, I found two packages providing boxed wrappers:

* [simd: simple interface to GHC's SIMD instructions](https://hackage.haskell.org/package/simd)
* [primitive-simd: SIMD data types and functions](https://hackage.haskell.org/package/primitive-simd)

The simd package is based on type families:

```haskell
module Data.SIMD.SIMD4 where

class SIMD4 a where
  data X4 a
  plusX4 :: X4 a -> X4 a -> X4 a
  indexArrayAsX4 :: ByteArray -> Int -> X4 a
  broadcastX4 :: a -> X4 a
  -- ...

class SIMD4 a => SIMD4Float a where
  divideX4 :: X4 a -> X4 a -> X4 a

instance (Show a, SIMD4 a) => Show (X4 a)
instance (Num a, SIMD4 a) => Num (X4 a)
instance (Fractional a, SIMD4Float a) => Fractional (X4 a)

instance SIMD4 Float
instance SIMD4 Double
-- ...

vectorizeUnboxedX4 :: (SIMD4 a, VU.Unbox a) => VU.Vector a -> VU.Vector (X4 a)
unVectorizeUnboxedX4 :: (SIMD4 a, VU.Unbox a) => VU.Vector (X4 a) -> VU.Vector a
vectorizeStorableX4 :: (SIMD4 a, Storable a, Storable (X4 a)) => VS.Vector a -> VS.Vector (X4 a)
unVectorizeStorableX4 :: (SIMD4 a, Storable a, Storable (X4 a)) => VS.Vector (X4 a) -> VS.Vector a
```

It provides type families based on the number of elements (`X4`, `X8`, `X16`).
One thing to note is that `X4 Double` is a wrapper for `DoubleX4#` and therefore requires AVX.

If you want to map over `Vector Float`, the number of elements must be a multiple of 4.

Let's see the other package, primitive-simd.

```haskell
module Data.Primitive.SIMD where

class (Num v, Real (Elem v)) => SIMDVector v where
  type Elem v
  type ElemTuple v
  broadcastVector :: Elem v -> v
  mapVector :: (Elem v -> Elem v) -> v -> v
  -- ...

data FloatX4
data FloatX8
data FloatX16
data DoubleX2
data DoubleX4
data DoubleX8
-- ...
```

It provides individual wrappers, not type families.

One interesting point is, `DoubleX4` does not always use `DoubleX4#` and can be configured to use two `DoubleX2#`.
It allows to use `DoubleX4` on SSE-only machines, or NEON-only Arm machines.

I like the idea of using type families like `X4`/`X8`/`X16`, and I like the idea of using two `DoubleX2#` for `X4 Double`/`DoubleX4`.
I'll come back to this point later.

## How GHC can be improved

In this section, we will consider what can be done to make GHC's SIMD primitives easier to use.

### More backends

Traditionally (on GHC &lt; 9.8), you had to

1. use LLVM backend
2. have an x86(-64) machine

to use SIMD primitives.
This situation was not great because

* you need to install LLVM toolchain in addition to GHC, and
* some of recent computers are not x86 (I'm writing this article with an Apple Silicon Mac).

If few backends support SIMD primitives, why would package authors want to use them?
There should be more backends that support SIMD primitives.

Recently, there is some progress on this area.
I'm going to describe them.

#### x86(-64) NCG

Currently, the most popular architecture for PCs is x86-64.
Therefore, it would be beneficial to make x86(-64) NCG support SIMD primitives.

Fortunately, GHC 9.12 will contain some support for SIMD with x86-64 NCG.
The work originally started as a GSoC project in 2018 and was once merged in 2019, but later reverted because of incomplete handling of register spilling.

* [GSoC final evaluation](https://gist.github.com/Abhiroop/9a2600f24f8139b7fc0742a2b4585bb3)
* [#7741: Add SIMD support to x86/x86_64 NCG · Issues · Glasgow Haskell Compiler / GHC · GitLab](https://gitlab.haskell.org/ghc/ghc/-/issues/7741)
* [!1306: AVX/SSE4 SIMD support for the native code generator · Merge requests · Glasgow Haskell Compiler / GHC · GitLab](https://gitlab.haskell.org/ghc/ghc/-/merge_requests/1306)
* [!1379: Back out SIMD NCG support · Merge requests · Glasgow Haskell Compiler / GHC · GitLab](https://gitlab.haskell.org/ghc/ghc/-/merge_requests/1379)

Then this year (2024), [@sheaf](https://gitlab.haskell.org/sheaf) and [@jaro](https://gitlab.haskell.org/jaro) continued the work and managed to get basic support of SIMD into x86(-64) NCG.

* [!12860: Add SIMD support to the X86 NCG · Merge requests · Glasgow Haskell Compiler / GHC · GitLab](https://gitlab.haskell.org/ghc/ghc/-/merge_requests/12860)
* [!12857: Add Int64X2 SIMD operations · Merge requests · Glasgow Haskell Compiler / GHC · GitLab](https://gitlab.haskell.org/ghc/ghc/-/merge_requests/12857)

The initial support was limited to `FloatX4#`, `DoubleX2#` (and packing/unpacking of `Int64X2#`) and some operations required SSE4.1 instructions (by default, GHC refuses to use SSE instructions beyond SSE2).
Since then, I have worked to make NCG emit SSE1 instructions for basic `FloatX4#` operations:

* [#25441: x86 NCG SIMD: Primitives like insertFloatX4# should be available without -msse4 · Issues · Glasgow Haskell Compiler / GHC · GitLab](https://gitlab.haskell.org/ghc/ghc/-/issues/25441)
* [!13542: x86 NCG SIMD: Lower packFloatX4#, insertFloatX4# and broadcastFloatX4# to SSE1 instructions · Merge requests · Glasgow Haskell Compiler / GHC · GitLab](https://gitlab.haskell.org/ghc/ghc/-/merge_requests/13542) (backported to GHC 9.12)

and now I'm working on to support packing/unpacking of other integer vector types:

* [#25487: x86 NCG SIMD: Implement pack/insert/broadcast/unpack for integer vectors · Issues · Glasgow Haskell Compiler / GHC · GitLab](https://gitlab.haskell.org/ghc/ghc/-/issues/25487)
* [!13602: Draft: x86 NCG SIMD: Support pack/insert/broadcast/unpack of 128-bit integer vectors · Merge requests · Glasgow Haskell Compiler / GHC · GitLab](https://gitlab.haskell.org/ghc/ghc/-/merge_requests/13602)

Although there are many tasks to do (integer arithmetic, 256-bit and 512-bit support via AVX/AVX-512/AVX10, better code generation, etc[^tasks]), I think the future is bright.

[^tasks]: If you are interested, see this ticket: [#25030: Meta-issue for SIMD support in the native code generator · Issues · Glasgow Haskell Compiler / GHC · GitLab](https://gitlab.haskell.org/ghc/ghc/-/issues/25030)

#### AArch64 LLVM/NCG

The next most popular architecture for PCs is probably AArch64 (Arm64).
Therefore, it should be supported as well.

AArch64 has 128-bit wide ASIMD (NEON) instructions and optional SVE instructions.
NEON's design fits GHC's, so it is the first target.

Supporting NEON via LLVM should be easy, and actually I did it in 2023:

* [#23410: Support 128-bit SIMD on AArch64 via LLVM backend · Issues · Glasgow Haskell Compiler / GHC · GitLab](https://gitlab.haskell.org/ghc/ghc/-/issues/23410)
* [!10456: Support 128-bit SIMD on AArch64 via LLVM backend · Merge requests · Glasgow Haskell Compiler / GHC · GitLab](https://gitlab.haskell.org/ghc/ghc/-/merge_requests/10456)

So GHC 9.8+ have support for SIMD on AArch64 via LLVM backend.
If you are using ghcup on macOS, you can enable LLVM backend (`-fllvm`) via the following command:

```
$ brew install llvm@15
$ OPT=$(brew --prefix llvm@15)/bin/opt LLC=$(brew --prefix llvm@15)/bin/llc ghcup install ghc --force 9.8.4
```

Support via NCG backend is also desirable, but it would be non-trivial task.

#### WebAssembly

Recent WebAssembly has 128-bit wide SIMD, so it would be good to have support for them.

There's an issue: [#22618: Add SIMD128 support to the wasm backend · Issues · Glasgow Haskell Compiler / GHC · GitLab](https://gitlab.haskell.org/ghc/ghc/-/issues/22618)

#### Others

Other targets may have some support for SIMD, but with some notes.

PowerPC has 128-bit AltiVec, but does not support double precision floating-points.
Should GHC emulate `DoubleX2#` via scalar operations, or choose not to support it?

32-bit Arm has NEON as an extension, but the floating-point vector operations does not support subnormals (they always use flush-to-zero mode).
Is this acceptable?

RISC-V has vector extension, but I don't know how close their design is to GHC's design.

Other backends (like via-C or JS) has no SIMD support.

### More primitives

The number of operations implemented in current GHC is limited.
Traditionally (GHC &le; 9.10), the available operations were the following:

* Basic ones
    * `pack`
    * `insert`
    * `unpack`
    * `broadcast`
    * `index`/`read`/`write` for arrays and pointers
* `plus`
* `minus`
* `times`
* `divide` for floating-points
* `quot`/`rem` for integers
    * Interestingly, there is no SIMD instructions for integer division on x86.
* `negate` for signed integers and floating-points

Recently, GHC 9.12 added `min`/`max`/FMA/`shuffle`.

Still, some operations that are frequently used are missing:

* `sqrt`
* `abs`
* Bitwise operations

In fact, LLVM backend can generate these instructions without dedicated vector primitives.
LLVM's optimizer would fuse the following code into single instruction:

```haskell
sqrtFloatX4# :: FloatX4# -> FloatX4#
sqrtFloatX4# v = case unpackFloatX4# v of
                   (# x0, x1, x2, x3 #) ->
                     packFloatX4# (# sqrtFloat# x0, sqrtFloat# x1, sqrtFloat# x2, sqrtFloat# x3 #)
```

Nevertheless, addition of vector primitives for these would be desirable because implementing such optimizations in NCG would not be pleasant.

In addition to these, primitives related to conditionals (or masks) are currently missing.
Comparison operations, logical operations, `select`, and so on.
I wonder if we should have types like `BoolX4#` not depending on element type, or have types like `FloatX4Mask#` for each vector type.

### FFI

Currently, vector types are allowed in FFI if `UnliftedFFITypes` extension is enabled.

We may want to support `vectorcall` calling convention on Windows, because the standard calling convention mandates all 128-bit vectors to be passed by reference.

## What a SIMD library should look like

### Boxed wrappers

SIMD primitives are a low-level feature and not for end-users.
As I said before, wrappers must be provided by a library.
How should a such library look like?

The idea of using type families per number of elements, like the simd package, sounds nice.
They have the good property of being a functor:

```haskell
-- pseudocode
data X4 a = X4 !a !a !a !a
```

The idea of using two `DoubleX2#` to represent a vector of four `Double`s, like the primitive-simd package, also sounds nice.

Therefore, our boxed wrapper would look like:

```haskell
data X4 a
data instance X4 Float = FloatX4 FloatX4#
#if MAX_VEC_LEN == 128
data instance X4 Double = DoubleX2X2 DoubleX2# DoubleX2#
#elif MAX_VEC_LEN >= 256
data instance X4 Double = DoubleX4 DoubleX4#
#endif
```

### Type classes

Let's consider the type classes.
We want to have instances like these:

```haskell
instance (??? a) => Num (X4 a)
instance (??? a) => Fractional (X4 a)
```

How should the superclass constraint look like?
One idea is to have classes for `X4`:

```haskell
class BroadcastX4 a where
  broadcastX4 :: a -> X4 a

instance BroadcastX4 Float
instance BroadcastX4 Double

class BroadcastX4 a => NumX4 a where
  plusX4 :: X4 a -> X4 a -> X4 a
  -- ...

instance NumX4 Float
instance NumX4 Double

instance (Num a, NumX4 a) => Num (X4 a) where
  (+) = plusX4
  -- ...
```

But the `X4` part could be generalized to other type families:

```haskell
class Broadcast f a where
  broadcast :: a -> f a

instance Broadcast X4 Float
instance Broadcast X4 Double
-- instances for X8, X16 could be added

class Broadcast f a => NumF f a where
  plusF :: f a -> f a -> f a
  minusF :: f a -> f a -> f a
  timesF :: f a -> f a -> f a
  absF :: f a -> f a
  signumF :: f a -> f a
  negateF :: f a -> f a

instance NumF X4 Float
instance NumF X4 Double
-- instances for X8, X16 could be added

instance (NumF X4 a, Num a) => Num (X4 a) where
  (+) = plusF
  -- ...
```

### Mapping a long vector

To read/write a (long) vector, let's define a `Storable`-like classes:

```haskell
class Storable a => StorableF f a where
  peekElemOffF :: Ptr a -> Int -> IO (f a) -- index in scalar elements
  pokeElemOffF :: Ptr a -> Int -> f a -> IO () -- index in scalar elements

instance StorableF X4 Float
instance StorableF X4 Double
```

With these, a `map` function that supports SIMD would look like:

```haskell
mapStorable :: (StorableF X4 a, StorableF X4 b) => (X4 a -> X4 b) -> (a -> b) -> VS.Vector a -> VS.Vector b
mapStorable fv f !v = unsafePerformIO $ do
  let !n = VS.length v
  !result <- VSM.unsafeNew n
  VS.unsafeWith v $ \ !inputPtr ->
    VSM.unsafeWith result $ \ !resultPtr -> do
      let loopVector !i | i + 4 > n = loopScalar i
                        | otherwise = do
                          !a <- peekElemOffF inputPtr i
                          pokeElemOffF resultPtr i (fv a)
                          loopVector (i + 4)
          loopScalar !i | i >= n = pure ()
                        | otherwise = do
                          !a <- peekElemOff inputPtr i
                          pokeElemOff resultPtr i (f a)
                          loopScalar (i + 1)
      loopVector 0
  VS.unsafeFreeze result
{-# INLINE mapStorable #-}
```

For simplicity, fusion-like optimizations are not implemented.

A use of this function would look like:

```haskell
mapStorable (\v -> (v + 1)^(10 :: Int)) (\x -> (x + 1)^(10 :: Int)) (VS.fromList [0..10] :: VS.Vector Float)
```

### Unifying the SIMD part and the scalar part

The `mapStorable` function that we just defined takes a function for the SIMD part `X4 a -> X4 b` and one for the scalar part `a -> b`.
This leads to code duplication.
Can we avoid it?

If the operation is `f x = (x + 1)^10`, the function type could be generalized to `Num a => a -> a`, and we could have:

```haskell
mapStorableNum :: (StorableF X4 a, Num a, NumF X4 a) => (forall v. Num v => v -> v) -> VS.Vector a -> VS.Vector a
mapStorableNum f = mapStorable f f
```

But then we need to many `map` functions to possible constraints (`Num`, `Fractional`, `Bits`, etc).
That is not desirable.

Let's consider another idea.
The scalar function is isomorphic to `Identity a -> Identity b`, so both the SIMD part and scalar part could be written as `f a -> f b`.
I mean, if we have a type class `SIMD` that have `X4` and `Identity` as instances, then we could write as:

```haskell
mapStorable' :: (StorableF X4 a, Storable X4 b) => (forall f. SIMD f => f a -> f b) -> VS.Vector a -> VS.Vector b
mapStorable' f = mapStorable f (runIdentity . f . Identity)
```

What should the `SIMD f` constraint be?
`f` should have instances like `Num (f Float)`, `Num (f Double)`.
Ideally, we would like to have `Num (f a)` for any `Num a`.
That is:

```haskell
class (forall a. Num a => Num (f a)) => SIMD f
instance SIMD Identity
instance SIMD X4
```

...except that it's impossible.
We defined `X4 a` only for `a ~ Float` and `a ~ Double`.
We are not going to have `X4 Integer`.
Let's add the additional constraint:

```haskell
class (forall a. (Num a, NumF X4 a) => Num (f a)) => SIMD f
instance SIMD Identity
instance SIMD X4
```

If we are going to have types for other element-counts like `X8` or `X16`, we can add them:

```haskell
class (forall a. (Num a, NumF X4 a, NumF X8 a, NumF X16 a) => Num (f a)) => SIMD f
instance SIMD Identity
instance SIMD X4
instance SIMD X8
instance SIMD X16
```

If we are going to support other type classes, we can add them:

```haskell
type NumElement a = (Num a, NumF X4 a, NumF X8 a, NumF X16 a)
type FractionalElement a = (Fractional a, FractionalF X4 a, FractionalF X8 a, FractionalF X16 a)
type FloatingElement a = (Floating a, FloatingF X4 a, FloatingF X8 a, FloatingF X16 a)
type MonoidElement a = (Monoid a, MonoidF X4 a, MonoidF X8 a, MonoidF X16 a)
class ( forall a. NumElement a => Num (f a)
      , forall a. FractionalElement a => Fractional (f a)
      , forall a. FloatingElement a => Floating (f a)
      , forall a. MonoidElement a => Monoid (f a)
      ) => SIMD f
instance SIMD Identity
instance SIMD X4
instance SIMD X8
instance SIMD X16
```

The user can write as

```haskell
f :: Num a => a -> a
f x = (x + 1)^(10 :: Int)

mapStorable' f (VS.fromList [0..10])
```

or

```haskell
g :: (SIMD f, NumElement a) => f a -> f a
g x = (x + 1)^(10 :: Int)

mapStorable' g (VS.fromList [0..10])
```

### A working example

I have a working example for SIMD at [haskell-dsl-example/simd](https://github.com/minoki/haskell-dsl-example/tree/main/simd).

```haskell
$ git clone https://github.com/minoki/haskell-dsl-example.git
$ cd haskell-dsl-example/simd
```

I use an Apple Silicon Mac, so I need to use GHC 9.8+ with LLVM backend enabled.

```
$ cabal configure -w ghc-9.8.4 --ghc-options=-fllvm
```

A successful run of `cabal run` look like:

```
$ cabal run
[1.0,1024.0,59049.0,1048576.0,9765625.0,6.0466176e7,2.8247526e8,1.0737418e9,3.4867843e9,1.0e10,2.5937424e10]
[1.0,1024.0,59049.0,1048576.0,9765625.0,6.0466176e7,2.8247526e8,1.0737418e9,3.4867843e9,1.0e10,2.5937424e10]
[1.0,1024.0,59049.0,1048576.0,9765625.0,6.0466176e7,2.8247526e8,1.0737418e9,3.4867843e9,1.0e10,2.5937424e10]
[1.0,1024.0,59049.0,1048576.0,9765625.0,6.0466176e7,2.8247526e8,1.0737418e9,3.4867843e9,1.0e10,2.5937424e10]
[1.0,1024.0,59049.0,1048576.0,9765625.0,6.0466176e7,2.82475249e8,1.073741824e9,3.486784401e9,1.0e10,2.5937424601e10]
[1.0,1024.0,59049.0,1048576.0,9765625.0,6.0466176e7,2.82475249e8,1.073741824e9,3.486784401e9,1.0e10,2.5937424601e10]
[1.0,1024.0,59049.0,1048576.0,9765625.0,6.0466176e7,2.82475249e8,1.073741824e9,3.486784401e9,1.0e10,2.5937424601e10]
[1.0,1024.0,59049.0,1048576.0,9765625.0,6.0466176e7,2.82475249e8,1.073741824e9,3.486784401e9,1.0e10,2.5937424601e10]
```

The benchmark is available at [simd/benchmark/Main.hs](https://github.com/minoki/haskell-dsl-example/blob/main/simd/benchmark/Main.hs).
I defined two functions to benchmark:

```haskell
f :: Num a => a -> a
f x = (x + 1)^(10 :: Int)
{-# INLINE f #-}

g :: Num a => a -> a
g x = let y = x + 1
          y2 = y * y
          y4 = y2 * y2
          y8 = y4 * y4
      in y8 * y2
{-# INLINE g #-}
```

Here is the benchmark result with Apple M4 Pro:

```
$ cabal bench -w ghc-9.8.4 --ghc-options=-fllvm -O2
benchmarking Float/f/scalar
time                 3.206 μs   (3.200 μs .. 3.211 μs)
                     1.000 R²   (1.000 R² .. 1.000 R²)
mean                 3.207 μs   (3.201 μs .. 3.215 μs)
std dev              22.70 ns   (15.40 ns .. 37.09 ns)

benchmarking Float/f/vector
time                 892.3 ns   (889.6 ns .. 894.8 ns)
                     1.000 R²   (1.000 R² .. 1.000 R²)
mean                 893.0 ns   (890.9 ns .. 894.5 ns)
std dev              6.179 ns   (4.750 ns .. 7.933 ns)

benchmarking Float/f/vector (unified)
time                 897.8 ns   (895.3 ns .. 900.2 ns)
                     1.000 R²   (1.000 R² .. 1.000 R²)
mean                 898.7 ns   (896.3 ns .. 902.2 ns)
std dev              9.800 ns   (6.691 ns .. 16.75 ns)

benchmarking Float/g/scalar
time                 3.215 μs   (3.211 μs .. 3.219 μs)
                     1.000 R²   (1.000 R² .. 1.000 R²)
mean                 3.213 μs   (3.210 μs .. 3.220 μs)
std dev              15.61 ns   (8.731 ns .. 28.17 ns)

benchmarking Float/g/vector
time                 893.0 ns   (890.7 ns .. 895.9 ns)
                     1.000 R²   (1.000 R² .. 1.000 R²)
mean                 894.2 ns   (892.1 ns .. 896.7 ns)
std dev              7.870 ns   (6.011 ns .. 11.68 ns)

benchmarking Float/g/vector (unified)
time                 901.2 ns   (897.6 ns .. 904.0 ns)
                     1.000 R²   (1.000 R² .. 1.000 R²)
mean                 898.0 ns   (895.9 ns .. 899.9 ns)
std dev              6.522 ns   (5.186 ns .. 8.885 ns)

benchmarking Double/f/scalar
time                 3.185 μs   (3.179 μs .. 3.193 μs)
                     1.000 R²   (1.000 R² .. 1.000 R²)
mean                 3.188 μs   (3.184 μs .. 3.199 μs)
std dev              21.43 ns   (11.81 ns .. 36.86 ns)

benchmarking Double/f/vector
time                 3.649 μs   (3.621 μs .. 3.674 μs)
                     1.000 R²   (0.999 R² .. 1.000 R²)
mean                 3.662 μs   (3.640 μs .. 3.674 μs)
std dev              54.79 ns   (39.52 ns .. 73.07 ns)
variance introduced by outliers: 13% (moderately inflated)

benchmarking Double/f/vector (unified)
time                 3.671 μs   (3.652 μs .. 3.688 μs)
                     1.000 R²   (1.000 R² .. 1.000 R²)
mean                 3.672 μs   (3.654 μs .. 3.683 μs)
std dev              45.43 ns   (27.65 ns .. 83.19 ns)

benchmarking Double/g/scalar
time                 3.221 μs   (3.214 μs .. 3.228 μs)
                     1.000 R²   (1.000 R² .. 1.000 R²)
mean                 3.220 μs   (3.215 μs .. 3.225 μs)
std dev              17.02 ns   (12.05 ns .. 28.72 ns)

benchmarking Double/g/vector
time                 1.658 μs   (1.656 μs .. 1.662 μs)
                     1.000 R²   (1.000 R² .. 1.000 R²)
mean                 1.662 μs   (1.659 μs .. 1.673 μs)
std dev              16.55 ns   (4.710 ns .. 36.25 ns)

benchmarking Double/g/vector (unified)
time                 1.661 μs   (1.658 μs .. 1.664 μs)
                     1.000 R²   (1.000 R² .. 1.000 R²)
mean                 1.658 μs   (1.656 μs .. 1.662 μs)
std dev              9.381 ns   (4.704 ns .. 17.62 ns)
```

The `Float` code runs 3.206/0.8923≈3.59 times faster if vectorized.
Good.

For the `Double` code, `f` does not run faster.
On the other hand, `g` runs 3.221/1.658≈1.94 times faster if vectorized.
Nice.

Here is results from another CPU, Ryzen 9 7940HS (Zen 4), run on Ubuntu on WSL2:

```
$ cabal bench -w ghc-9.8.4 --ghc-options=-fllvm -O2
benchmarking Float/f/scalar
time                 4.190 μs   (4.154 μs .. 4.231 μs)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 4.201 μs   (4.172 μs .. 4.261 μs)
std dev              139.4 ns   (77.76 ns .. 238.0 ns)
variance introduced by outliers: 42% (moderately inflated)

benchmarking Float/f/vector
time                 1.288 μs   (1.276 μs .. 1.302 μs)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 1.293 μs   (1.282 μs .. 1.308 μs)
std dev              42.72 ns   (30.33 ns .. 65.09 ns)
variance introduced by outliers: 45% (moderately inflated)

benchmarking Float/f/vector (unified)
time                 1.290 μs   (1.279 μs .. 1.301 μs)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 1.291 μs   (1.280 μs .. 1.304 μs)
std dev              42.05 ns   (30.81 ns .. 60.24 ns)
variance introduced by outliers: 45% (moderately inflated)

benchmarking Float/g/scalar
time                 4.187 μs   (4.148 μs .. 4.235 μs)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 4.200 μs   (4.165 μs .. 4.257 μs)
std dev              149.5 ns   (109.6 ns .. 213.1 ns)
variance introduced by outliers: 46% (moderately inflated)

benchmarking Float/g/vector
time                 1.359 μs   (1.350 μs .. 1.370 μs)
                     1.000 R²   (0.999 R² .. 1.000 R²)
mean                 1.363 μs   (1.355 μs .. 1.375 μs)
std dev              32.39 ns   (21.39 ns .. 48.15 ns)
variance introduced by outliers: 30% (moderately inflated)

benchmarking Float/g/vector (unified)
time                 1.366 μs   (1.354 μs .. 1.378 μs)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 1.365 μs   (1.356 μs .. 1.378 μs)
std dev              35.55 ns   (26.86 ns .. 46.75 ns)
variance introduced by outliers: 34% (moderately inflated)

benchmarking Double/f/scalar
time                 4.214 μs   (4.183 μs .. 4.246 μs)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 4.225 μs   (4.195 μs .. 4.280 μs)
std dev              131.8 ns   (87.25 ns .. 222.4 ns)
variance introduced by outliers: 39% (moderately inflated)

benchmarking Double/f/vector
time                 2.159 μs   (2.140 μs .. 2.182 μs)
                     0.999 R²   (0.998 R² .. 0.999 R²)
mean                 2.208 μs   (2.180 μs .. 2.253 μs)
std dev              116.2 ns   (77.12 ns .. 170.5 ns)
variance introduced by outliers: 67% (severely inflated)

benchmarking Double/f/vector (unified)
time                 2.146 μs   (2.131 μs .. 2.165 μs)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 2.153 μs   (2.138 μs .. 2.172 μs)
std dev              57.85 ns   (44.10 ns .. 86.56 ns)
variance introduced by outliers: 34% (moderately inflated)

benchmarking Double/g/scalar
time                 4.192 μs   (4.159 μs .. 4.225 μs)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 4.195 μs   (4.171 μs .. 4.234 μs)
std dev              100.5 ns   (76.68 ns .. 130.0 ns)
variance introduced by outliers: 27% (moderately inflated)

benchmarking Double/g/vector
time                 2.187 μs   (2.171 μs .. 2.205 μs)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 2.192 μs   (2.177 μs .. 2.215 μs)
std dev              61.57 ns   (44.70 ns .. 88.71 ns)
variance introduced by outliers: 36% (moderately inflated)

benchmarking Double/g/vector (unified)
time                 2.181 μs   (2.159 μs .. 2.207 μs)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 2.193 μs   (2.176 μs .. 2.225 μs)
std dev              75.01 ns   (45.34 ns .. 123.4 ns)
variance introduced by outliers: 46% (moderately inflated)
```

The `Float` code runs 4.190/1.288≈3.25 times faster if vectorized.

For the `Double` code, `f` does not run faster.
On the other hand, `g` runs 4.192/2.181≈1.92 times faster if vectorized.

Let's try NCG backend from GHC 9.12.

```
$ cabal bench -w ghc-9.12.0.20241128 --builddir=dist-ncg -O2 --allow-newer
benchmarking Float/f/scalar
time                 37.44 μs   (37.14 μs .. 37.76 μs)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 37.34 μs   (37.09 μs .. 37.86 μs)
std dev              1.080 μs   (665.7 ns .. 1.866 μs)
variance introduced by outliers: 29% (moderately inflated)

benchmarking Float/f/vector
time                 7.048 μs   (6.981 μs .. 7.101 μs)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 7.047 μs   (6.995 μs .. 7.129 μs)
std dev              212.2 ns   (133.8 ns .. 324.3 ns)
variance introduced by outliers: 36% (moderately inflated)

benchmarking Float/f/vector (unified)
time                 7.014 μs   (6.959 μs .. 7.071 μs)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 7.034 μs   (6.988 μs .. 7.108 μs)
std dev              186.6 ns   (146.7 ns .. 252.1 ns)
variance introduced by outliers: 30% (moderately inflated)

benchmarking Float/g/scalar
time                 13.74 μs   (13.61 μs .. 13.88 μs)
                     0.999 R²   (0.998 R² .. 0.999 R²)
mean                 13.54 μs   (13.40 μs .. 13.66 μs)
std dev              458.4 ns   (356.4 ns .. 604.5 ns)
variance introduced by outliers: 40% (moderately inflated)

benchmarking Float/g/vector
time                 1.408 μs   (1.388 μs .. 1.441 μs)
                     0.996 R²   (0.993 R² .. 0.998 R²)
mean                 1.478 μs   (1.448 μs .. 1.529 μs)
std dev              136.4 ns   (83.51 ns .. 203.8 ns)
variance introduced by outliers: 87% (severely inflated)

benchmarking Float/g/vector (unified)
time                 1.180 μs   (1.171 μs .. 1.191 μs)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 1.176 μs   (1.169 μs .. 1.187 μs)
std dev              32.02 ns   (21.83 ns .. 50.35 ns)
variance introduced by outliers: 36% (moderately inflated)

benchmarking Double/f/scalar
time                 38.91 μs   (38.57 μs .. 39.29 μs)
                     0.999 R²   (0.998 R² .. 0.999 R²)
mean                 38.59 μs   (38.25 μs .. 39.06 μs)
std dev              1.361 μs   (1.070 μs .. 1.724 μs)
variance introduced by outliers: 38% (moderately inflated)

benchmarking Double/f/vector
time                 8.440 μs   (8.366 μs .. 8.537 μs)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 8.456 μs   (8.399 μs .. 8.541 μs)
std dev              232.4 ns   (160.3 ns .. 309.1 ns)
variance introduced by outliers: 32% (moderately inflated)

benchmarking Double/f/vector (unified)
time                 8.149 μs   (8.068 μs .. 8.241 μs)
                     0.999 R²   (0.999 R² .. 0.999 R²)
mean                 8.199 μs   (8.127 μs .. 8.305 μs)
std dev              292.6 ns   (229.5 ns .. 450.0 ns)
variance introduced by outliers: 44% (moderately inflated)

benchmarking Double/g/scalar
time                 13.86 μs   (13.73 μs .. 13.98 μs)
                     0.999 R²   (0.999 R² .. 0.999 R²)
mean                 13.71 μs   (13.62 μs .. 13.85 μs)
std dev              374.7 ns   (285.7 ns .. 525.6 ns)
variance introduced by outliers: 30% (moderately inflated)

benchmarking Double/g/vector
time                 2.641 μs   (2.621 μs .. 2.664 μs)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 2.652 μs   (2.631 μs .. 2.682 μs)
std dev              83.22 ns   (60.53 ns .. 118.6 ns)
variance introduced by outliers: 41% (moderately inflated)

benchmarking Double/g/vector (unified)
time                 2.221 μs   (2.201 μs .. 2.244 μs)
                     0.999 R²   (0.999 R² .. 1.000 R²)
mean                 2.231 μs   (2.213 μs .. 2.267 μs)
std dev              80.59 ns   (46.99 ns .. 135.7 ns)
variance introduced by outliers: 48% (moderately inflated)
```

`Float/f` runs 37.44/7.014≈5.34 times faster if vectorized.
`Float/g` runs 13.74/1.180≈11.6 times faster if vectorized.

`Double/f` runs 38.91/8.149≈4.77 times faster if vectorized.
`Double/g` runs 13.86/2.221≈6.24 times faster if vectorized.

With NCG backend, the degree of improvement is more than the number of parallelization (4 or 2).
Maybe the scalar code is not optimized well.

## Using CPU features conditionally

We have some SIMD support in x86 NCG backend, and we can write a library for vectorization.
Is that all?
No, we have another problem.

SSE2 is always available on x86-64, but other extensions like SSE4.1, AVX2, and AVX-512 are not available on all CPUs (or emulators).
Therefore, if a program wants to use AVX2, either it should be built exclusively for CPUs supporting AVX2, or conditionally use AVX2 by looking at CPUID.
Currently, GHC supports only the first option, via flags like `-mavx2`.

This may be fine for programs that only need to run on a specific CPU, but not for libraries that need to run on many CPUs.

Therefore, it is desirable to have a mechanism to enable a CPU feature on a function (or execution path) basis rather than on a per-module basis, and a mechanism to switch the code to be executed by looking at CPUID.

As a prior art, GNU C has an attribute for selecting the target: `__attribute__((target("avx2")))`.
Therefore, we could have something similar like:

```haskell
{-# TARGET "avx2" #-}
foo :: X8 Int32 -> X8 Int32 -> X8 Int32
foo x y = 2 * x + y
```

There is a related issue:
[#15876: Function versioning instead of compilation flags... · Issues · Glasgow Haskell Compiler / GHC · GitLab](https://gitlab.haskell.org/ghc/ghc/-/issues/15876)

Now, I'm wondering if nullary type classes (or type families) could be used to represent CPU features.
I mean, something like

```haskell
-- Using a nullary type class

class HasAVX2
-- -mavx2 would make the constraint `HasAVX2` available on that module

plusInt32X8# :: HasAVX2 => Int32X8# -> Int32X8# -> Int32X8#

type Dict :: Constraint -> Type
data Dict c = c => Dict

-- modules that don't have -mavx2 enabled can use `detectAVX2` to conditionally enable AVX2
detectAVX2 :: Maybe (Dict HasAVX2)
detectAVX2 = ... -- use CPUID to detect AVX2
```

or

```haskell
-- Using a nullary type family

type family AVX2 :: Bool
-- -mavx2 would make the equation `AVX2 ~ True` available on that module

plusInt32X8# :: AVX2 ~ True => Int32X8# -> Int32X8# -> Int32X8#

data SBool (b :: Bool) where
  STrue :: SBool True
  SFalse :: SBool False

-- modules that don't have -mavx2 enabled can use `detectAVX2` to conditionally enable AVX2
detectAVX2 :: SBool AVX2
detectAVX2 = ... -- use CPUID to detect AVX2
```

This is just an idea (I'm no expert on GHC's internals), and I don't know if it works well when you want cross-platform support (x86, AArch64, ...).

## Conclusion

Although not complete, there is some progress on GHC's SIMD support.

I will work on improving x86 NCG's support for SIMD on my spare time.
